{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "from data_analysis import data_frame\n",
    "\n",
    "data_frame['dis'][ data_frame['dis'] == 'NA' ] = 5\n",
    "data_frame['obs'][ data_frame['obs'] == 'NA' ] = 3\n",
    "\n",
    "\n",
    "from data_analysis import p_dis, p_obs, p_prs\n",
    "\n",
    "significant_dis = p_dis <= 0.05/900\n",
    "significant_obs = p_obs <= 0.05/900\n",
    "significant_prs = p_prs <= 0.05/900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "similarity_matrix_xyz = np.zeros((1600,1600))\n",
    "\n",
    "similarity_matrix_xy = np.zeros((1600,1600))\n",
    "similarity_matrix_xz = np.zeros((1600,1600))\n",
    "similarity_matrix_yz = np.zeros((1600,1600))\n",
    "\n",
    "similarity_matrix_dis = np.zeros((1600,1600))\n",
    "similarity_matrix_obs = np.zeros((1600,1600))\n",
    "\n",
    "similarity_matrix_left_hand = np.zeros((1600,1600))\n",
    "\n",
    "for x in tqdm(range(1600)):\n",
    "    for y in range(1600):\n",
    "        X = data_frame['data'][x]\n",
    "        Y = data_frame['data'][y]\n",
    "        \n",
    "        X_flipped = data_frame['data'][x]\n",
    "        \n",
    "        X_flipped[1] *=  -1\n",
    "        \n",
    "        diff = (X - Y)**2\n",
    "        \n",
    "        sim_xyz = diff.mean()\n",
    "        \n",
    "        sim_left = ((X_flipped - Y)**2).mean()\n",
    "        similarity_matrix_left_hand[x,y] = sim_left\n",
    "        \n",
    "        sim_xy = diff[:, (0,1)].mean()\n",
    "        sim_xz = diff[:, (0,2)].mean()\n",
    "        sim_yz = diff[:, (1,2)].mean()\n",
    "        \n",
    "        sim_dis = diff[significant_dis].mean()\n",
    "        sim_obs = diff[significant_obs].mean()\n",
    "        \n",
    "        \n",
    "        similarity_matrix_xyz[x,y] = sim_xyz\n",
    "        \n",
    "        similarity_matrix_xy[x,y] = sim_xy\n",
    "        similarity_matrix_xz[x,y] = sim_xz\n",
    "        similarity_matrix_yz[x,y] = sim_yz\n",
    "        \n",
    "        similarity_matrix_dis[x,y] = sim_dis\n",
    "        similarity_matrix_obs[x,y] = sim_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = similarity_matrix_left_hand\n",
    "\n",
    "most_similar = np.argsort(similarity_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = [1, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59]\n",
    "\n",
    "def k_nearest(X, not_indexes, s_parameter, y_labels, most_similar):\n",
    "    sim_list_ = most_similar[X]\n",
    "    \n",
    "    sim_list = []\n",
    "    \n",
    "    for e in sim_list_:\n",
    "        if len(sim_list) >= s_parameter:\n",
    "            break\n",
    "        if e not in not_indexes:\n",
    "            sim_list += [e]\n",
    "    else:\n",
    "        print(\"Der skete en fejl her din spade\")\n",
    "        \n",
    "    y_list = y_labels[sim_list]\n",
    "        \n",
    "    vals_, index_, counts_ = np.unique(y_list, return_counts=True, return_index=True)\n",
    "    \n",
    "    vals = vals_[np.argsort(index_)]\n",
    "    counts = counts_[np.argsort(index_)]\n",
    "    \n",
    "    index = np.argmax(counts)\n",
    "    \n",
    "    return vals[index]\n",
    "\n",
    "\n",
    "def train_model(not_indexes, s_parameter, y_labels):\n",
    "    return lambda X: k_nearest(X, not_indexes, s_parameter, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate matrix of p-values for the different classifiers being identical\n",
    "\n",
    "y_labels = np.array(data_frame['obs'])\n",
    "similarity_matrices = [similarity_matrix_xyz, similarity_matrix_xy, similarity_matrix_xz, similarity_matrix_yz, similarity_matrix_dis, similarity_matrix_obs, similarity_matrix_left_hand]\n",
    "classifiers = []\n",
    "for similarity_matrix in similarity_matrices:\n",
    "    classifiers.append(np.argsort(similarity_matrix, axis = 1))\n",
    "\n",
    "classifier_p_values = np.zeros((len(classifiers), len(classifiers)))\n",
    "for i in range(len(classifiers)):\n",
    "    for j in range(len(classifiers)):\n",
    "        mc_nemar_matrix = np.zeros((2, 2))\n",
    "        for k in range(1600):\n",
    "            i_correct = k_nearest(k, [k], 20, y_labels, classifiers[i]) == y_labels[k]\n",
    "            j_correct = k_nearest(k, [k], 20, y_labels, classifiers[j]) == y_labels[k]\n",
    "            mc_nemar_matrix[int(i_correct), int(j_correct)] += 1\n",
    "        classifier_p_values[i, j] = mcnemar(mc_nemar_matrix, correction = False).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.zeros(len(classifiers))\n",
    "for i in range(len(classifiers)):\n",
    "    correct_count = 0\n",
    "    for k in range(1600):\n",
    "        correct_count += k_nearest(k, [k], 20, y_labels, classifiers[i]) == y_labels[k]\n",
    "    accuracies[i] = correct_count / 1600\n",
    "\n",
    "labels = np.array([\"xyz\", \"xy\", \"xz\", \"yz\", \"dis\", \"obs\", \"left hand\"])\n",
    "for label, accuracy in zip(labels[np.argsort(accuracies)], np.sort(accuracies)):\n",
    "    print(f\"{label}: {str(accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label      Mean      Sig\n",
      "-------------------------\n",
      "dis        0.45      a\n",
      "xy         0.47562   b\n",
      "xz         0.80188   c\n",
      "xyz        0.81562   def\n",
      "left hand  0.81562   def\n",
      "yz         0.8175    def\n",
      "obs        0.83688   g\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.45    , 0.475625, 0.801875, 0.815625, 0.815625, 0.8175  ,\n",
       "        0.836875]),\n",
       " array(['dis', 'xy', 'xz', 'xyz', 'left hand', 'yz', 'obs'], dtype='<U9'),\n",
       " ['a', 'b', 'c', 'def', 'def', 'def', 'g']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_stat_significance_table(labels, means, p_value_matrix, sig_level = 0.05):\n",
    "    \"\"\"Generates a table showing the sorted means and whether they are statistcally different.\\n\n",
    "       Parameters:\n",
    "        - labels: The names of the variables.\n",
    "        - means: The means of the variables.\n",
    "        - p_value_matrix: An n x n matrix with p-values for whether two variables have the same mean.\n",
    "        - sig_level: The significance level where we can't reject the null hypothesis.\"\"\"\n",
    "\n",
    "    sorted_means = np.sort(means)\n",
    "    sorted_labels = labels[np.argsort(means)]\n",
    "    significance_strings = []\n",
    "    sorted_p_value_matrix = p_value_matrix[np.argsort(means)][:, np.argsort(means)]\n",
    "    \n",
    "    for i in range(len(sorted_p_value_matrix)):\n",
    "        significance_string = \"\"\n",
    "        for j in range(len(sorted_p_value_matrix)):\n",
    "            if sorted_p_value_matrix[i, j] >= sig_level:\n",
    "                significance_string += string.ascii_lowercase[j]\n",
    "        significance_strings.append(significance_string)\n",
    "    \n",
    "    max_label_len = max(max([len(label) for label in sorted_labels]), 5)\n",
    "    max_mean_len = max(max([len(str(mean)) for mean in sorted_means]), 4)\n",
    "    table_string = \"Label\" + \" \" * (max_label_len - 3) + \"Mean\" + \" \" * (max_mean_len - 2) + \"Sig\\n\"\n",
    "    table_string += \"-\" * len(table_string) + \"\\n\"\n",
    "    for i in range(len(sorted_labels)):\n",
    "        table_string += sorted_labels[i] + \" \" * (max_label_len + 2 - len(sorted_labels[i])) + str(round(sorted_means[i], 5)) + \" \" * (max_mean_len + 2 - len(str(round(sorted_means[i], 5)))) + significance_strings[i] + \"\\n\"\n",
    "    print(table_string)\n",
    "\n",
    "    return [sorted_means, sorted_labels, significance_strings]\n",
    "\n",
    "generate_stat_significance_table(np.array(labels), accuracies, classifier_p_values, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.ascii_lowercase)\n",
    "x = np.array([4,2,5,7,3,4,5,2])\n",
    "y = np.sort(x)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"xyz\", \"xy\", \"xz\", \"yz\", \"dis\", \"obs\", \"left hand\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(classifier_p_values)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, fontsize = 50)\n",
    "ax.set_yticklabels(labels, fontsize = 50)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two layer cross validation\n",
    "\n",
    "# inits\n",
    "y_labels = np.array(data_frame['obs'])\n",
    "\n",
    "generalization_error = 0\n",
    "generalization_error_s = np.zeros(len(model_parameters))\n",
    "all_generalization_error_s = []\n",
    "best_test_error = []\n",
    "best_test_lambda = []\n",
    "\n",
    "\n",
    "for fold1 in tqdm(range(10)):\n",
    "    X_par_prs = data_frame[ data_frame['prs'] != fold1 ]['prs']\n",
    "    \n",
    "    par_indexes = np.arange(1600)[ data_frame['prs'] != fold1 ]\n",
    "    test_indexes = np.arange(1600)[ data_frame['prs'] == fold1 ]\n",
    "    \n",
    "    fold2values = list(range(10))\n",
    "    fold2values.remove(fold1)\n",
    "    \n",
    "    for fold2 in fold2values:\n",
    "        train_indexes = par_indexes[ X_par_prs != fold2 ]\n",
    "        val_indexes = par_indexes[ X_par_prs == fold2 ]\n",
    "        \n",
    "        for s_count, s_parameter in enumerate(model_parameters):\n",
    "            not_indexes = np.hstack((test_indexes, val_indexes))\n",
    "            model_s = train_model(not_indexes, s_parameter, y_labels)\n",
    "            \n",
    "            y_val = y_labels[val_indexes]\n",
    "            y_val_estimates = np.array([model_s(X) for X in val_indexes])\n",
    "            \n",
    "            generalization_error_s[s_count] += len(val_indexes)/len(par_indexes) * np.mean(y_val_estimates != y_val)\n",
    "            \n",
    "    \n",
    "    # get and save generalization error for model s + select best model\n",
    "    best_model_index = np.argmin(generalization_error_s)\n",
    "    best_model_parameters = model_parameters[best_model_index]\n",
    "    \n",
    "    all_generalization_error_s.append(generalization_error_s)\n",
    "    generalization_error_s = np.zeros(len(model_parameters))\n",
    "    \n",
    "    # calculate generalization error\n",
    "    best_model = train_model(test_indexes, best_model_parameters, y_labels)\n",
    "    \n",
    "    y_test = y_labels[test_indexes]\n",
    "    y_test_estimates = np.array([best_model(X) for X in test_indexes])\n",
    "    \n",
    "    best_test_lambda.append(best_model_parameters)\n",
    "    best_test_error.append(np.mean(y_test_estimates != y_test))\n",
    "    \n",
    "    generalization_error += len(test_indexes)/len(data_frame) * np.mean(y_test_estimates != y_test)\n",
    "\n",
    "\n",
    "average_generalization_error_s = np.mean(np.array(all_generalization_error_s), axis=0)\n",
    "\n",
    "parameter_overview = list(zip(model_parameters, average_generalization_error_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices\n",
    "\n",
    "model_parameter = 3\n",
    "\n",
    "\n",
    "exp2distance = {i: i%5 for i in range(15)}\n",
    "exp2distance[15] = 5\n",
    "\n",
    "exp2obstacle = {i: i%3 for i in range(15)}\n",
    "exp2obstacle[15] = 3\n",
    "\n",
    "confussion_experiment = np.zeros((16,16))\n",
    "confussion_distance = np.zeros((6,6))\n",
    "confussion_obstacle = np.zeros((4,4))\n",
    "\n",
    "# inits\n",
    "y_labels = np.array(data_frame['exp'])\n",
    "\n",
    "for fold1 in tqdm(range(10)):\n",
    "    train_indexes = np.arange(1600)[ data_frame['prs'] != fold1 ]\n",
    "    test_indexes = np.arange(1600)[ data_frame['prs'] == fold1 ]\n",
    "    \n",
    "    not_indexes = test_indexes\n",
    "    model_s = train_model(not_indexes, model_parameter, y_labels)\n",
    "    \n",
    "    y_val = y_labels[val_indexes]\n",
    "    y_val_estimates = np.array([model_s(X) for X in val_indexes])\n",
    "    \n",
    "    for y_pred, y_true in zip(y_val_estimates, y_val):\n",
    "        confussion_experiment[y_pred, y_true] += 1\n",
    "        confussion_distance[exp2distance[y_pred], exp2distance[y_true]] += 1\n",
    "        confussion_obstacle[exp2obstacle[y_pred], exp2obstacle[y_true]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "\n",
    "cms = [confussion_experiment, confussion_distance, confussion_obstacle]\n",
    "all_labels = [range(1,17), ['15cm', '22.5cm', '30cm', '37.5cm', '45cm', 'NA'], ['S', 'M', 'T', 'NA']]\n",
    "\n",
    "for cm, labels in zip(cms, all_labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39f7b1e1acb982adffb7db11462de7cdfb80f348d07483f59786aa90f88d75ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
